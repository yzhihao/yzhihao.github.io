---
layout: post
title:  图像生成模型概述-GAN
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-5-5T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# 图像生成模型概述-GAN

## Maximum likelihood estimation

To simplify the discussion somewhat, we will focus on generative models that work via the principle of maximum likelihood.Some generative models do not use maximum likelihood by default, but can be made to do so (GANs fall into this category).


也就是说GAN也是以极大似然来建模的一种生成模型。而与庞大的真实数据相比，***概率生成模型的参数个数要远远小于数据的数量。因此，在训练过程中，生成模型会被强迫去发现数据背后更为简单的统计规律，从而能够生成这些数据。**



<img src="{{ site.img_path }}/Machine Learning/generation_model_cv.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

注意两点，（1）是在计算极大似然的时候，用log的方式（2）是优化极大似然都可以看所是在缩小KL散度。


## A taxonomy of deep generative models

If we restrict our attention to deep generative models that work by **maximizing the likelihood**, we can compare several models by contrasting the ways that they compute either the likelihood and its gradients, or approximations to these quantities. As mentioned earlier, many of these models are often used with principles other than maximum likelihood, but we can examine the maximum likelihood variant of each of them in order to reduce the amount of distracting differences between the methods

<img src="{{ site.img_path }}/Machine Learning/generation_model_cv1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

如上图，我们关注用极大似然的方式建模的话，有上面的一些种类，每一类都有他们的优缺点。

## Explicit density models

The main difficulty present in explicit density models is **designing a model that can capture all of the complexity of the data to be generated while still maintaining computational tractability.** There are two different strategies used to confront this challenge: **(1) careful construction of models whose structure guarantees their tractability, as described in section next, and (2) models that admit tractable approximations to the likelihood and its gradients, as described in section next**

explicit density models 又分为tractable explicit models和逼近的explicit model，怎么理解呢，tractable explicit model通常可以直接通过数学方法来建模求解，而基于逼近的explicit model通常无法直接对数据分布进行建模，可以利用数学里的一些近似方法来做数据建模， 通常基于逼近的explicit model分为确定性（变分方法：如VAE的lower bound）和随机性的方法（马尔科夫链蒙特卡洛方法）。

VAE lower bound：

<img src="{{ site.img_path }}/Machine Learning/generation_model_cv2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

VAE依靠的是传统的**概率图模型的框架**，通过一些适当的**联合分布**的概率逼近，简化整个学习过程，使得所学习到的模型能够很好地解释所观测到的数据。

## Implicit density models

Some models can be trained **without even needing to explicitly define a density functions.** These models instead offer a way to train the model while interacting only indirectly with p model , usually by sampling from it.

## Comparing GANs to other generative models

生成对抗式网络（GAN）能够有效地解决很多生成式方法的缺点，主要包括：

1. 并行产生samples；
2. 生成式函数的限制少，如无需合适马尔科夫采样的数据分布（Boltzmann machines），生成式函数无需可逆、latent 3. code需与sample同维度（nonlinear ICA）；
4. 无需马尔科夫链的方法（Boltzmann machines， GSNs）；
5. 相对于VAE的方法，无需variational bound；
6. GAN比其他方法一般来说性能更好。

At the same time, GANs have taken on a new disadvantage: training them requires finding the Nash equilibrium of a game, which is a more difficult problem than optimizing an objective function.

当然，GAN还存在缺点，比如说容易模型崩塌，训练不稳定等。


## GAN的发展现状和研究方向。

1. Non-convergence
2. Evaluation of generative models
3. Discrete outputs
4. Semi-supervised learning
5. Using the code
6. Developing connections to reinforcement learning
7. Plug and Play Generative Networks

总的研究方向有上面的几个。












