---
layout: post
title: 机器学习有关的数学基础知识-概率
desc: 我的博客
keywords: 'blog,Machine Learning,AI'
date: 2017-2-14T00:00:00.000Z
categories:
  - Machine Learning
tags:
  - Machine Learning
  - AI
icon: fa-book
---


## 目录
**欢迎在文章下方评论，建议用电脑看**

* 目录
{:toc}

# 机器学习有关的数学基础知识-概率

## 条件概率

* 当A,B都代表是一个事件的时候，条件概率的计算：`p(AB)=P(B)P(A|B)`。这个是高中就学过的，不多说！

## 贝叶斯概率
学习机器学习，就不得不学贝叶斯这个东西啦，首先，不要怕，他就是一个条件概率，表面上理解起来很简单！

要学习贝叶斯概率，首先就是要知道全概率公式。全概率公式如下：
<img src="{{ site.img_path }}/Machine Learning/machine_sta2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

不要着急，如果理解不了下面有例子。

* 贝叶斯概率分布由分子就是条件概率计算结果，分布就是全概率的就算结果。如下图：

<img src="{{ site.img_path }}/Machine Learning/machine_sta1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

* 下面来好好看看这个公式：若`P(A|B) = P(A)P(B|A) / P(B)`，则

P(A)：先验概率。即：在的得到新数据前某一假设的概率。
P(A|B)：后验概率。即：在看到新数据后，要计算的该假设的概率。
P(B|A)：似然度。即：在该假设下，得到这一数据的概率。
P(B)：标准化常量。即：在任何假设下得到这一数据的概率。


好了上例子：

<img src="{{ site.img_path }}/Machine Learning/machine_sta3.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/machine_sta4.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>





## 离散型/连续型随机变量

* 随机变量X：为一映射,其自变量具有随机性，就是一个函数，注意：自变量是事件，因变量是事件发生的概率

* 若随机变量X的取值为有限个或可数,就称X离散,自变量-事件离散。


## 常见的离散型随机变量分布


### 0-1分布/贝努利分布

随机变量X的只可能取0，1 两个值，只有两个可能结果的试验，故称为**两点分布**有时也称为**贝努利分布**.

>可以理解为只有一次实验，两种可能的值的分布


### 二项分布

就是k重伯努利实验的分布中，p发生K次的概率

>注意，0-1分布/贝努利分布是二项分布的一种特殊情况，就是n=1的时候







### 泊松分布.

* 如果某事件以固定强度λ,随机且独立地出现，该事件在单位时间内出现的次数（个数）可以看成是服从泊松分布.

例如：

        某人一天内收到的微信的数量
        来到某公共汽车站的乘客
        某放射性物质发射出的粒子
        显微镜下某区域中的白血球

### 二项分布和泊松分布
![]()
即 当 n >10, p < 0.1 时 , 二项分布 B ( n , p ) 可以用泊松分布pi( np ) 来近似.

>在这里，可以这样看：就是当事件多样性多，概率都较低的时候就是泊松分布，否则就是二项分布

### 几何分布

* 在重复多次的贝努里试验中, 试验进行到某种结果出现第一次为止此时的试验总次数服从几何分布. 如：射击, 首次击中目标时射击的次数;




### 分布函数

* 就是关于随机变量的函数，也就是关于x<某个值(在离散型表示的就是边界，在连续表示的就是某一个点)的函数

* 一般地，离散型随机变量的分布函数为阶梯函数，



>离散型和连续型分布函数的理解见



## 概率密度和离散型随机变量

* 两个是相对应的，其性质是相对应的。

* 两个是一样的，但随机变量表示离散时的事件概率，概率密度表示在某个值的概率。



>**随机变量就是：事件到概率的映射，密度函数也一样，只不过是极限**

>**随机变量函数就是用随机变量做自变量**



>连续型结合直线上的例子，注意的是F(x)表示的是概率，面积总和为1，F（x）表示的是概率密度可以大于1，这个值大小满足积分为1，并大于0就可，可以用**课件：概率密度和连续型变量的例1**

>F(x)一定是一个连续函数



## 均匀分布

* 直观理解：x落到a，b等长即落入中的的任意子度等区间上是可能的.

* 概率计算就可以推导出是长度之比





## 指数分布

* 概率密度函数是一个指数阶的

* 重要的性质，是无记忆性的，这个性质就是唯一的。也就是说看是不是服从指数分布就可以通过是否有无记忆性来指出

打电话的例子：

<img src="{{ site.img_path }}/Machine Learning/machine_sta6.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>
答案：
<img src="{{ site.img_path }}/Machine Learning/machine_sta7.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>



## 高斯分布
关于高斯分布的性质如下图：
![]()


* 一般的高斯分布可以转化（(x-u)/(方差)）为标准正太分布，然后再去查表算概率值

### 正态分布的用途：
自然界和人类社会中很多现象可以看做正态分布

    如: 人的生理尺寸(身高、体重);
    医学检验指标(红细胞数、血小板);
    测量误差;等等
    多个随机变量的和可以用正态分布来近似
    如: 注册MOOC的某位同学完成所有作业的时间;
    二项分布; 等等
    (By 中心极限定理)

## 中心极限定理

* 中心极限定理是概率论中的一组定理。中央极限定理说明，大量相互独立的随机变量，其均值的分布以正态分布为极限。这组定理是数理统计学和误差分析的理论基础，指出了大量随机变量之和近似服从正态分布的条件。

![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/10/HistPropOfHeads.png/300px-HistPropOfHeads.png)
本图描绘了多次抛掷硬币实验中出现正面的平均比率，每次实验均抛掷了大量硬币。


## 联合概率和边缘概率

* 条件概率和联合概率的区别，看例子**例5，离散型多元概率**

而对于条件概率和联合概率的区别请看下面的例子：

![]()
![]()

//TODO待截图

## 概率论中的独立同分布?请分别解释独立、同分布及独立同分布.

(1)独立就是每次抽样之间是没有关系的,不会相互影响

就像我抛色子每次抛到几就是几这就是独立的

但若我要两次抛的和大于8,其余的不算,那么第一次抛和第二次抛就不独立了,因为第二次抛的时候结果是和第一次相关的

(2)同分布的意思就是每次抽样,样本都服从同样的一个分布

抛色子每次得到任意点数的概率都是1/6,这就是同分布的

但若我第一次抛一个六面的色子,第二次抛一个正12面体的色子,就不再是同分布了

(3)独立同分布,也叫i,i,d,就是每次抽样之间独立而且同分布的意思

追问：

同分布是指服从同一分布函数么？是的。





## 联合分布

随机变量X和Y的联合分布函数是设(X,Y)是二维随机变量，对于任意实数x,y，二元函数：F(x,y) = P{(X<=x) 交 (Y<=y)} => P(X<=x, Y<=y)称为二维随机变量(X,Y)的分布函数。



## 二元高斯分布

*　关于二元的条件和边际分布，请查看慕课

*　注意边际分布和条件分布也是一个一元的高斯分布。



## 期望
probability_math
* 连续型的期望可以类比离散型的期望，不过就是一个是求和，一个是积分





## 极大似然估计

先上一个例子：


<img src="{{ site.img_path }}/Machine Learning/probability_math.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

答：

<img src="{{ site.img_path }}/Machine Learning/probability_math1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

* 就是先给出一定的样本结果，通过结果来极大的估计参数值,需要注意的是，这里是知道分布而求参数（与EM算法要区别开），使得得到这样结果的可能性最大

* 如果是离散型的就是随机事件的概率，如果是连续型的可能就是一个参数**具体看课件：极大似然估计**

给出连续型和离散型的标准定义：

<img src="{{ site.img_path }}/Machine Learning/probability_math2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/probability_math3.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


再举个例子：

<img src="{{ site.img_path }}/Machine Learning/probability_math4.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/probability_math5.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>




其实对L(θ)取对数将其变成连加的H(θ)还有一个原因：通常L(θ)中每个p(xi; θ)都很小，许多很小的数字相乘起来在计算机里很容易造成浮点数下溢，所以对其取对数将其变成连加的

>讲到极大似然就想起EM算法，他们之间有很大的联系

> EM算法是为了解决“最大似然估计”中更复杂的情形而存在的。
这里“极大似然估计中更复杂的情形”是什么情形呢？
我们知道极大似然估计是求解实现结果的最佳参数θ，但极大似然估计需要面临的概率分布只有一个或者知道结果是通过哪个概率分布实现的，只不过你不知道这个概率分布的参数。而如果概率分布有多个呢或者你不知道结果是通过哪个概率分布实现的？于是别说去确定“这些概率分布”的最佳参数了，我们连最终结果是根据哪个概率分布得出来的都不知道，这就是EM算法要面临的情况了。有关EM算法更多看我的那篇博文[EM算法]()

[混合概率模型](http://wenku.baidu.com/link?url=Bh9Zhx3xPnT_yLtWfe6GHgUugl3W78iF2qVBvoo8q1BBhcJzsrdmYWqJo5wgfKsBjSY_7QqsagHx2vwZnMKyPJl3dl0xY_5Wue-XhyjMQT3)



参考资料：

《深度学习》-Bengio<br>
《统计学习方法》-李航<br>
《机器学习》-周志华<br>
《机器学习实战》-Peter Harrington<br>
斯坦福大学公开课-机器学习<br>
网上的各位大牛的博文<br>

















