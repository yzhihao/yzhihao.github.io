# 神经网络（基本概念）

现在，我们一起来了解下最近都大火的神经网络（neural network），它看起来，听起来很高大上，但是，事实上，理解起来也就这样啦，下面来一起看下吧！

>以下大多笔记来自cs231n，一个非常不错的deep learning课，值得一看，奉上[链接](http://cs231n.stanford.edu/)

# 作为线性分类器的单个神经元
* 只要在神经元的输出端有一个合适的损失函数，就能让单个神经元变成一个线性分类器。因此说，那些&**线性的分类器本身就是一个单层神经网络**
* **但注意，对于非线性的模型：**SVM和神经网络走了两条不同的道路：神经网络通过**多个隐层的方法来实现非线性的函数，有一些理论支持（比如说带隐层的神经网络可以模拟任何函数），但是目前而言还不是非常完备；SVM则采用了kernel trick的方法，这个在理论上面比较完备（RKHS，简单地说就是一个泛函的线性空间）。**两者各有好坏，神经网络最近的好处是网络设计可以很灵活，但是老被人说跳大神；SVM的理论的确漂亮，但是kernel设计不是那么容易，所以最近没有那么热了。

>注意：说神经网络多少层数的时候一般不包括输入层。
>在神经网络中的激活主要讲的是梯度的更新的激活


## why deep neural network not fat neural network??

<img src="{{ site.img_path }}/Machine Learning/why_deep_neural_network.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/why_deep_neural_network1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

A shallow network has less number of hidden layers. **While there are studies that a shallow network can fit any function, it will need to be really fat. That causes the number of parameters to increase a lot.**

There are quite conclusive results that a deep network can fit functions better with less parameters than a shallow network.

## 表达能力（capacity）

* 表达能力：就是指对某个神经网络的判别能力（泛化误差，训练误差之类的来衡量）
* 可以认为它们定义了一个由一系列函数组成的函数族，网络的权重就是每个函数的参数。
* 神经网络可以近似任何**连续函数**。至于为什么，可以简单的理解为当隐层数量越高，它就能把样本空间分为更多部分，这样对于分类来说就可以更加的准确，但要注意的是防止过拟合，具体看一看下这篇:[神经网络为什么能够无限逼近任意连续函数？](https://zhuanlan.zhihu.com/p/25590725)
* **更大网络总是更好的这一事实。然而更大容量的模型一定要和更强的正则化（比如更高的权重衰减）配合，否则它们就会过拟合。**

## 机器学习的模型中为什么加入bias?

<img src="{{ site.img_path }}/Machine Learning/bais1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

<img src="{{ site.img_path }}/Machine Learning/bais2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

## 正则化 Regularization:

<img src="{{ site.img_path }}/Machine Learning/regulazation1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

>注意：**L1正则化会让权重向量在最优化的过程中变得稀疏（即非常接近0）;L2正则化可以直观理解为它对于大数值的权重向量进行严厉惩罚，倾向于更加分散的权重向量。**

**l1,l2正则化的图像化：**

<img src="{{ site.img_path }}/Machine Learning/27regulaztion.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>

随机失活（Dropout）：**Dropout可以看作是Bagging的极限形式，每个模型都在当一情况中训练，同时模型的每个参数都经过与其他模型共享参数，从而高度正则化。**在训练过程中，随机失活也可以被认为是对完整的神经网络抽样出一些子集，每次基于输入数据只更新子网络的参数（然而，数量巨大的子网络们并不是相互独立的，因为它们都共享参数）。在**测试过程中不使用随机失活，可以理解为是对数量巨大的子网络们做了模型集成（model ensemble），以此来计算出一个平均的预测。**关于这个，可以看下[Batch-Normalization and dropout](https://yzhihao.github.io/machine%20learning/2017/03/29/Batch-Normalization-and-dropout.html)中的dropout部分



>需要注意的是，**在正则化的时候，bais是不需要正则化的，不然可能会导致欠拟合！**具体看[这篇博文](https://yzhihao.github.io/machine%20learning/2017/03/24/Deep-Learning-Deep-Feedforward-Networks.html#regularization)







## 数据预处理

### 为什么要预处理：

简单的从二维来理解，首先，图像数据是高度相关的，假设其分布如下图a所示(简化为2维)。由于初始化的时候，我们的参数一般都是0均值的，因此开始的拟合y=Wx+b，基本过原点附近(因为b接近于零)，如图b红色虚线。因此，网络需要经过多次学习才能逐步达到如紫色实线的拟合，即收敛的比较慢。如果我们对输入数据先作减均值操作，如图c，显然可以加快学习。更进一步的，我们对数据再进行去相关操作，使得数据更加容易区分，这样又会加快训练，如图d。

<img src="{{ site.img_path }}/Machine Learning/neural_network_base1.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


### 基础预处理方法

* 归一化处理
    1. 均值减法（Mean subtraction）:它对数据中**每个独立特征减去平均值，**从几何上可以理解为在每个维度上都将数据云的中心都迁移到原点。（就是每个特征数据减去其相应特征的平均值）
    2. 归一化（Normalization）;先对数据做零中心化（zero-centered）处理，然后每个维度都除以其标准差，实现代码为`X /= np.std(X, axis=0)。`


<img src="{{ site.img_path }}/Machine Learning/base_neural_network_s1.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


**左边：**原始的2维输入数据。**中间：**在每个维度上都减去平均值后得到零中心化数据，现在数据云是以原点为中心的。**右边：**每个维度都除以其标准差来调整其数值范围。红色的线指出了数据各维度的数值范围，在中间的零中心化数据的数值范围不同，但在右边归一化数据中数值范围相同。

* PCA和白化（Whitening）是另一种预处理形式

    1. 白化（Whitening）:白化操作的输入是特征基准上的数据，然后对**每个维度除以其特征值来对数值范围进行归一化**。该变换的几何解释是：如果数据服从多变量的高斯分布，那么经过白化后，数据的分布将会是一个**均值为零，且协方差相等**的矩阵
    2. 特征向量是按照特征值的大小排列的。我们可以利用这个性质来对数据降维，只要使用前面的小部分特征向量，丢弃掉那些包含的数据没有方差的维度。 这个操作也被称为[主成分分析（ Principal Component Analysis）](https://yzhihao.github.io/machine%20learning/2017/02/11/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7-%E4%BA%8C.html#pca) 简称PCA）降维


<img src="{{ site.img_path }}/Machine Learning/base_neural_network_s2.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


(中间是pca，右边是白化),

### 需要注意的是:

1. 对比与上面的中心化，pca有点类似，但是不同的是，pca把数据变换到了数据协方差矩阵的基准轴上（协方差矩阵变成对角阵），也就是说他是轴对称个的，但简单的零中心化，它不是轴对称个的；还有PCA是一种降维的预处理，而零中心化并不是。
2. 在上面的pca然后白化可以数据零均值（pca）和单位方差，弱相关性。
3. **常见错误:**进行预处理很重要的一点是：任何预处理策略（比如数据均值）都只能在训练集数据上进行计算，算法训练完毕后再应用到验证集或者测试集上。例如，如果先计算整个数据集图像的平均值然后每张图片都减去平均值，最后将整个数据集分成训练/验证/测试集，那么这个做法是错误的。应该怎么做呢？应该先分成训练/验证/测试集，**只是从训练集中求图片平均值，然后各个集（训练/验证/测试集）中的图像再减去这个平均值。**

##  Batch Normalization

**Batch Normalization就是在每一层的wx+b和f(wx+b)之间加一个归一化（将wx+b归一化成：均值为0，方差为1；**但在原论文中，作者为了计算的稳定性，加了两个参数将数据又还原回去了，这两个参数也是需要训练的。）层，说白了，就是对每一层的数据都预处理一次。方便直观感受，上张图：


<img src="{{ site.img_path }}/Machine Learning/neural_network_base2.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


这个方法可以进一步加速收敛，因此学习率可以适当增大，加快训练速度；过拟合现象可以得倒一定程度的缓解，所以可以不用Dropout或用较低的Dropout，而且可以减小L2正则化系数，训练速度又再一次得到了提升。即Batch Normalization可以降低我们对正则化的依赖程度。

现在的深度神经网络基本都会用到Batch Normalization：其做法是让激活数据在训练开始前通过一个网络，网络处理数据使其服从标准高斯分布。在全连接之后做的操作

**一句话总结：批量归一化可以理解为在网络的每一层之前都做预处理，只是这种操作以另一种方式与网络集成在了一起。搞定！**

>还有要注意的是，Batch Normalization和pca加白化有点类似，结果都是可以零均值加上单位方差，可以使得数据弱相关，但是在深度神经网络中，**我们一般不要pca加白化，原因就是白化需要计算整个训练集的协方差矩阵、求逆等操作，计算量很大，此外，反向传播时，白化操作不一定可导。**
>最后，可以看下[Batch-Normalization and dropout](https://yzhihao.github.io/machine%20learning/2017/03/29/Batch-Normalization-and-dropout.html#batch-normalization)中的Batch-Normalization部分

## 权重初始化
* 全零初始化是禁止的。因为：**如果网络中的每个神经元都计算出同样的输出，然后它们就会在反向传播中计算出同样的梯度，从而进行同样的参数更新。换句话说，如果权重被初始化为同样的值导致在隐层的敏感值和梯度权值都相等，神经元之间就失去了不对称性的源头。(会导致：所有的隐单元都是相同的. 学习失败)**

下面是两个网上资料，关于为什么不能全零初始化：

[全零初始化-解释1](http://www.cnblogs.com/dengdan890730/p/5865558.html)

[讨论全零初始化-解释2](https://www.zhihu.com/question/36068411/answer/95670563?from=profile_answer_card)



小随机数初始化： 产生的数字为**以零为平均值，单位标准差的高斯分布。**使用这种方式，每个神经元的权重矩阵都从多维度高斯分布中随机初始化。所以神经元在输入空间内指向不同的方向。代码为`w = np.random.randn(n) 	* sqrt(2.0/n)`。**这个形式是神经网络算法使用ReLU神经元时的当前最佳推荐。**

稀疏初始化（Sparse initialization）:每个神经元都同下一层固定数目的神经元随机连接（其权重数值由一个小的高斯分布生成）

偏置（biases）的初始化。**通常将偏置初始化为0，这是因为随机小数值权重矩阵已经打破了对称性。**对于ReLU非线性激活函数，有研究人员喜欢使用如0.01这样的小数值常量作为所有偏置的初始值，这是因为他们认为这样做能让所有的ReLU单元一开始就激活，这样就能保存并传播一些梯度。然而，这样做是不是总是能提高算法性能并不清楚（有时候实验结果反而显示性能更差），所以通常还是使用0来初始化偏置参数。


## 理解激励函数

### sigmoid非线性函数

* 优点：
	1. 比较好的解释性
* 缺点：
	1. Sigmoid函数饱和使梯度消失。sigmoid神经元有一个不好的特性，就是当神经元的激活**在接近0或1处时会饱和**：在这些区域，梯度几乎为0。（信号可以理解为信号）
	2. 输出不是零中心的，这一情况将影响梯度下降的运作，因为如果输入神经元的数据总是正数，那么关于w的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这样梯度下降权重更新时出现z字型的下降。这样收敛会变得异常的慢。（**这也是为什么要一直保持为数据的0中心化**）-----**但这个问题比较小**
	3. `exp（）`在深度神经网络时候相比就比较慢


<img src="{{ site.img_path }}/Machine Learning/neural_network_base3.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


###	Tanh非线性函数
* 优点
	1. 它的输出是零中心的。因此，在实际操作中，tanh非线性函数比sigmoid非线性函数更受欢迎。
* 缺点
	1. 和Sigmoid函数一样，饱和使梯度消失。


<img src="{{ site.img_path }}/Machine Learning/neural_network_base4.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


### ReLU

* 优点
	1. ReLU对于随机梯度下降的收敛有巨大的加速作用（ Krizhevsky 等的论文指出有6倍之多）。据称这是**由它的线性，非饱和的公式**导致的
	2. **现在大部分的DNN用的激活函数，至于为什么，可以看下这个[paper-ImageNet](https://yzhihao.github.io/machine%20learning/2017/02/24/paper-ImageNet.html)**
* 缺点
	1. 当x是小于0的时候，那么从此所以流过这个神经元的梯度将都变成0。
	2. 这个ReLU单元在训练中将不可逆转的死亡，因为这导致了数据多样化的丢失。


<img src="{{ site.img_path }}/Machine Learning/neural_network_base5.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


###	Leaky ReLU
* 优点
	1. 非饱和的公式
	2. Leaky ReLU是为解决“ReLU死亡”问题的尝试
* 缺点
	1. 有些研究者的论文指出这个激活函数表现很不错，但是其效果并不是很稳定

>Kaiming He等人在2015年发布的论文Delving Deep into Rectifiers中介绍了一种新方法PReLU，把负区间上的斜率当做每个神经元中的一个参数。然而该激活函数在在不同任务中均有益处的一致性并没有特别清晰。


<img src="{{ site.img_path }}/Machine Learning/neural_network_base6.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


### ELU
指数线性单元（Exponential Linear Units, ELU）
ELU的公式为：

<img src="{{ site.img_path }}/Machine Learning/neural_network_base7.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


ELU.png
函数曲线如下：

<img src="{{ site.img_path }}/Machine Learning/neural_network_base8.jpg" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


###	Maxout
* Maxout是对ReLU和leaky ReLU的一般化归纳

* 优点
	1. 拥有ReLU单元的所有优点（线性操作和不饱和），而没有它的缺点（死亡的ReLU单元）
* 缺点
	1.每个神经元的参数数量增加了一倍，这就导致整体参数的数量激增。难训练,容易过拟合


### 怎么用激活函数

“那么该用那种呢？”**用ReLU非线性函数。注意设置好学习率，**(如果学习率设置得太高，可能会发现网络中40%的神经元都会死掉（在整个训练集中这些神经元都不会被激活）。通过合理设置学习率，这种情况的发生概率会降低。),*解决方案：*或许可以监控你的网络中死亡的神经元占的比例。如果单元死亡问题困扰你，就试试Leaky ReLU或者Maxout，不要再用sigmoid了。也可以试试tanh，但是其效果应该不如ReLU或者Maxout。

## 交叉熵
一个非常常见的，非常漂亮的成本函数是“交叉熵”（cross-entropy）。交叉熵产生于信息论里面的信息压缩编码技术，但是它后来演变成为从博弈论到机器学习等其他领域里的重要技术手段。它的定义如下：

<img src="{{ site.img_path }}/Machine Learning/neural_network_base9.png" alt="header1" style="height:auto!important;width:auto%;max-width:1020px;"/>


y 是我们预测的概率分布, y' 是实际的分布（我们输入的one-hot vector)

交叉熵是正的，并且当所有输入x的输出都能接近期望输出y的话，交叉熵的值将会接近 0。这两个特征在直觉上我们都会觉得它适合做代价函数。事实上，我们的均方代价函数也同时满足这两个特征。这对于交叉熵来说是一个好消息。而且交叉熵有另一个均方代价函数不具备的特征，它能够避免学习速率降低的情况。

因为MSE（均方误差）不会出太大问题、同时也基本不能很好地解决问题,而折叶损失函数不能很好的描述概率和局部目标化（local objective）的问题，故一般都用这个交叉熵作为损失函数。

>在现在的深度神经网络中，大多数情况下，我们使用的就是交叉熵这个代价函数
>[详细了解交叉熵](https://hit-scir.gitbooks.io/neural-networks-and-deep-learning-zh_cn/content/chap3/c3s1.html)




### 参数维数
1. 对于全链接网络，参数的数量：If network has sj units in layer j and sj+1 units in layer j+1, then Θ(j) will be of dimension sj+1×(sj+1).
2. 对于卷积神经网络和循环神经网络，参数是在空间上和时间上共享的，这个下面再说！





















